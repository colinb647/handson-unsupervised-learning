{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, pickle, gzip\n",
    "import datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Prep'''\n",
    "from sklearn import preprocessing as pp \n",
    "\n",
    "'''Algos'''\n",
    "##import tensorflow as tf\n",
    "## import tensorflow.contrib.tensorboard.plugins.projector as projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "current_path = os.getcwd()\n",
    "file = '/datasets/mnist_data/mnist.pkl.gz'\n",
    "\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shape of datasets\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_validation: \", X_validation.shape)\n",
    "print(\"Shape of y_validation: \", y_validation.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train), \\\n",
    "                         len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation), \\\n",
    "                   len(X_train)+len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the training matrix\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the labels\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_digit(example):\n",
    "    label = y_train.loc[0]\n",
    "    image = X_train.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first digit\n",
    "view_digit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(series):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(series)+1))\n",
    "    return label_binarizer.transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(originalSeries, newSeries):\n",
    "    label_binarizer = pp.LabelBinarizer()\n",
    "    label_binarizer.fit(range(max(originalSeries)+1))\n",
    "    return label_binarizer.inverse_transform(newSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot vectors for the labels\n",
    "y_train_oneHot = one_hot(y_train)\n",
    "y_validation_oneHot = one_hot(y_validation)\n",
    "y_test_oneHot = one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one-hot vector for example 0, which is the number 5\n",
    "y_train_oneHot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model using TensorFlow\n",
    "x_train, x_test = np.reshape(X_train.values, (50000,28,28))/ 255.0, np.reshape(X_test.values, (10000,28,28)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=10, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 784\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Variance Captured by 784 principal components\n",
    "print(\"Variance Explained by all 784 principal components: \", \\\n",
    "      sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Variance Captured by X principal components\n",
    "importanceOfPrincipalComponents = \\\n",
    "    pd.DataFrame(data=pca.explained_variance_ratio_)\n",
    "importanceOfPrincipalComponents = importanceOfPrincipalComponents.T\n",
    "\n",
    "print('Variance Captured by First 10 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:9].sum(axis=1).values)\n",
    "print('Variance Captured by First 20 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:19].sum(axis=1).values)\n",
    "print('Variance Captured by First 50 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:49].sum(axis=1).values)\n",
    "print('Variance Captured by First 100 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:99].sum(axis=1).values)\n",
    "print('Variance Captured by First 200 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:199].sum(axis=1).values)\n",
    "print('Variance Captured by First 300 Principal Components: ',\n",
    "      importanceOfPrincipalComponents.loc[:,0:299].sum(axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "sns.barplot(data=importanceOfPrincipalComponents.loc[:,0:9],color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        def scatterPlot(xDF, yDF, algoName):\n",
    "            tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "            tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "            tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "            sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "                       data=tempDF, fit_reg=False)\n",
    "            ax = plt.gca()\n",
    "            ax.set_title(\"Separation of Observations using \"+algoName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    X_train_scatter = pd.DataFrame(data=X_train.loc[:,[350,406]], index=X_train.index)\n",
    "    X_train_scatter = pd.concat((X_train_scatter,y_train), axis=1, join=\"inner\")\n",
    "    X_train_scatter.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", data=X_train_scatter, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations Using Original Feature Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "log_dir = current_path+\"/logs/\"\n",
    "!mkdir ./logs/\n",
    "X_train_PCA.to_csv(log_dir+'data.tsv', sep = '\\t', index=False)\n",
    "y_train.to_csv(log_dir+'labels.tsv', sep = '\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# TensorFlow Variable from data\n",
    "df_pca = X_train_PCA.values\n",
    "embedding_var = tf.Variable(df_pca, name=\"PCA\")\n",
    "summary_writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# Specify where you find the metadata\n",
    "metadata_path =  log_dir+\"labels.tsv\"\n",
    "embedding.metadata_path = metadata_path\n",
    "\n",
    "# Say that you want to visualise the embeddings\n",
    "projector.visualize_embeddings(summary_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = log_dir + \"model.ckpt\"\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver([embedding_var])\n",
    "    sess.run(embedding_var.initializer)\n",
    "    saver.save(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/Users/ankurpatel/Documents/handson-unsupervised-learning/logs --port=8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_components = 784\n",
    "batch_size = None\n",
    "\n",
    "incrementalPCA = IncrementalPCA(n_components=n_components, \\\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)\n",
    "X_train_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)\n",
    "\n",
    "X_validation_incrementalPCA = incrementalPCA.transform(X_validation)\n",
    "X_validation_incrementalPCA = \\\n",
    "    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_incrementalPCA, y_train, \"Incremental PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "n_components = 100\n",
    "alpha = 0.0001\n",
    "random_state = 2018\n",
    "n_jobs = -1\n",
    "\n",
    "sparsePCA = SparsePCA(n_components=n_components, \\\n",
    "                alpha=alpha, random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "sparsePCA.fit(X_train.loc[:10000,:])\n",
    "X_train_sparsePCA = sparsePCA.transform(X_train)\n",
    "X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)\n",
    "\n",
    "X_validation_sparsePCA = sparsePCA.transform(X_validation)\n",
    "X_validation_sparsePCA = \\\n",
    "    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_sparsePCA, y_train, \"Sparse PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "n_components = 100\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "random_state = 2018\n",
    "n_jobs = 1\n",
    "\n",
    "kernelPCA = KernelPCA(n_components=n_components, kernel=kernel, \\\n",
    "                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "kernelPCA.fit(X_train.loc[:10000,:])\n",
    "X_train_kernelPCA = kernelPCA.transform(X_train)\n",
    "X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)\n",
    "\n",
    "X_validation_kernelPCA = kernelPCA.transform(X_validation)\n",
    "X_validation_kernelPCA = \\\n",
    "    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_kernelPCA, y_train, \"Kernel PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "n_components = 200\n",
    "algorithm = 'randomized'\n",
    "n_iter = 5\n",
    "random_state = 2018\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, algorithm=algorithm, \\\n",
    "                   n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)\n",
    "\n",
    "X_validation_svd = svd.transform(X_validation)\n",
    "X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_svd, y_train, \"Singular Value Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Random Projection\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "eps = 0.5\n",
    "random_state = 2018\n",
    "\n",
    "GRP = GaussianRandomProjection(n_components=n_components, eps=eps, \\\n",
    "                               random_state=random_state)\n",
    "\n",
    "X_train_GRP = GRP.fit_transform(X_train)\n",
    "X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)\n",
    "\n",
    "X_validation_GRP = GRP.transform(X_validation)\n",
    "X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_GRP, y_train, \"Gaussian Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Random Projection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "n_components = 'auto'\n",
    "density = 'auto'\n",
    "eps = 0.5\n",
    "dense_output = False\n",
    "random_state = 2018\n",
    "\n",
    "SRP = SparseRandomProjection(n_components=n_components, \\\n",
    "        density=density, eps=eps, dense_output=dense_output, \\\n",
    "        random_state=random_state)\n",
    "\n",
    "X_train_SRP = SRP.fit_transform(X_train)\n",
    "X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)\n",
    "\n",
    "X_validation_SRP = SRP.transform(X_validation)\n",
    "X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_SRP, y_train, \"Sparse Random Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isomap\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "n_neighbors = 5\n",
    "n_components = 10\n",
    "n_jobs = 4\n",
    "\n",
    "isomap = Isomap(n_neighbors=n_neighbors, \\\n",
    "                n_components=n_components, n_jobs=n_jobs)\n",
    "\n",
    "isomap.fit(X_train.loc[0:5000,:])\n",
    "X_train_isomap = isomap.transform(X_train)\n",
    "X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)\n",
    "\n",
    "X_validation_isomap = isomap.transform(X_validation)\n",
    "X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \\\n",
    "                                   index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_isomap, y_train, \"Isomap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidimensional Scaling\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "n_components = 2\n",
    "n_init = 12\n",
    "max_iter = 1200\n",
    "metric = True\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "mds = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \\\n",
    "          metric=metric, n_jobs=n_jobs, random_state=random_state)\n",
    "\n",
    "X_train_mds = mds.fit_transform(X_train.loc[0:1000,:])\n",
    "X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[0:1001])\n",
    "\n",
    "scatterPlot(X_train_mds, y_train, \"Multidimensional Scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Linear Embedding (LLE)\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "method = 'modified'\n",
    "n_jobs = 4\n",
    "random_state = 2018\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, \\\n",
    "        n_components=n_components, method=method, \\\n",
    "        random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "lle.fit(X_train.loc[0:5000,:])\n",
    "X_train_lle = lle.transform(X_train)\n",
    "X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)\n",
    "\n",
    "X_validation_lle = lle.transform(X_validation)\n",
    "X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_lle, y_train, \"Locally Linear Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "n_components = 2\n",
    "learning_rate = 300\n",
    "perplexity = 30\n",
    "early_exaggeration = 12\n",
    "init = 'random'\n",
    "random_state = 2018\n",
    "\n",
    "tSNE = TSNE(n_components=n_components, learning_rate=learning_rate, \\\n",
    "            perplexity=perplexity, early_exaggeration=early_exaggeration, \\\n",
    "            init=init, random_state=random_state)\n",
    "\n",
    "X_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:5000,:9])\n",
    "X_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:5001])\n",
    "\n",
    "scatterPlot(X_train_tSNE, y_train, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch dictionary learning\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "\n",
    "n_components = 50\n",
    "alpha = 1\n",
    "batch_size = 200\n",
    "n_iter = 25\n",
    "random_state = 2018\n",
    "\n",
    "miniBatchDictLearning = MiniBatchDictionaryLearning( \\\n",
    "                        n_components=n_components, alpha=alpha, \\\n",
    "                        batch_size=batch_size, n_iter=n_iter, \\\n",
    "                        random_state=random_state)\n",
    "\n",
    "miniBatchDictLearning.fit(X_train.loc[:,:10000])\n",
    "X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)\n",
    "X_train_miniBatchDictLearning = pd.DataFrame( \\\n",
    "    data=X_train_miniBatchDictLearning, index=train_index)\n",
    "\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    miniBatchDictLearning.transform(X_validation)\n",
    "X_validation_miniBatchDictLearning = \\\n",
    "    pd.DataFrame(data=X_validation_miniBatchDictLearning, \\\n",
    "    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_miniBatchDictLearning, y_train, \\\n",
    "            \"Mini-batch Dictionary Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent Component Analysis\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "n_components = 25\n",
    "algorithm = 'parallel'\n",
    "whiten = True\n",
    "max_iter = 100\n",
    "random_state = 2018\n",
    "\n",
    "fastICA = FastICA(n_components=n_components, algorithm=algorithm, \\\n",
    "                  whiten=whiten, max_iter=max_iter, random_state=random_state)\n",
    "\n",
    "X_train_fastICA = fastICA.fit_transform(X_train)\n",
    "X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)\n",
    "\n",
    "X_validation_fastICA = fastICA.transform(X_validation)\n",
    "X_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \\\n",
    "                                    index=validation_index)\n",
    "\n",
    "scatterPlot(X_train_fastICA, y_train, \"Independent Component Analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
