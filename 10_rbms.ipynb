{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "from datetime import datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MovieLens 20M Dataset\n",
    "20,000,263 ratings\n",
    "27,278 movies\n",
    "138,493 users\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "current_path = os.getcwd()\n",
    "file = '/datasets/movielens_data/ratings.csv'\n",
    "ratingDF = pd.read_csv(current_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fields into appropriate data types\n",
    "ratingDF.userId = ratingDF.userId.astype(str).astype(int)\n",
    "ratingDF.movieId = ratingDF.movieId.astype(str).astype(int)\n",
    "ratingDF.rating = ratingDF.rating.astype(str).astype(float)\n",
    "ratingDF.timestamp = ratingDF.timestamp.apply(lambda x: \\\n",
    "                    datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store DataFrame as pickle for faster loading in the future\n",
    "pickle_file = '\\\\datasets\\\\movielens_data\\\\ratingPickle'\n",
    "ratingDF.to_pickle(current_path + pickle_file)\n",
    "ratingDF = pd.read_pickle(current_path + pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "ratingDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics on full dataset\n",
    "\n",
    "# 20,000,263 ratings\n",
    "# 26,744 unique movies\n",
    "# 138,493 unique users\n",
    "# 144 ratings on average per user\n",
    "\n",
    "n_users = ratingDF.userId.unique().shape[0]\n",
    "n_movies = ratingDF.movieId.unique().shape[0]\n",
    "n_ratings = len(ratingDF)\n",
    "avg_ratings_per_user = n_ratings/n_users\n",
    "\n",
    "print('Number of unique users: ', n_users)\n",
    "print('Number of unique movies: ', n_movies)\n",
    "print('Number of total ratings: ', n_ratings)\n",
    "print('Average number of ratings per user: ', avg_ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce size of dataset by taking top 1000 movies\n",
    "movieIndex = ratingDF.groupby(\"movieId\").count().sort_values(by= \\\n",
    "                \"rating\",ascending=False)[0:1000].index\n",
    "ratingDFX2 = ratingDF[ratingDF.movieId.isin(movieIndex)]\n",
    "ratingDFX2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce size of dataset by sampling 1000 users\n",
    "userIndex = ratingDFX2.groupby(\"userId\").count().sort_values(by= \\\n",
    "    \"rating\",ascending=False).sample(n=1000, random_state=2018).index\n",
    "ratingDFX3 = ratingDFX2[ratingDFX2.userId.isin(userIndex)]\n",
    "ratingDFX3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reindex movie ID\n",
    "movies = ratingDFX3.movieId.unique()\n",
    "moviesDF = pd.DataFrame(data=movies,columns=['originalMovieId'])\n",
    "moviesDF['newMovieId'] = moviesDF.index+1\n",
    "moviesDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reindex user ID\n",
    "users = ratingDFX3.userId.unique()\n",
    "usersDF = pd.DataFrame(data=users,columns=['originalUserId'])\n",
    "usersDF['newUserId'] = usersDF.index+1\n",
    "usersDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate newly merged DataFrame\n",
    "ratingDFX3 = ratingDFX3.merge(moviesDF,left_on='movieId', \\\n",
    "                              right_on='originalMovieId')\n",
    "ratingDFX3.drop(labels='originalMovieId', axis=1, inplace=True)\n",
    "\n",
    "ratingDFX3 = ratingDFX3.merge(usersDF,left_on='userId', \\\n",
    "                              right_on='originalUserId')\n",
    "ratingDFX3.drop(labels='originalUserId', axis=1, inplace=True)\n",
    "ratingDFX3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle\n",
    "pickle_file = '\\\\datasets\\\\movielens_data\\\\ratingReducedPickle'\n",
    "ratingDFX3.to_pickle(current_path + pickle_file)\n",
    "ratingDFX3 = pd.read_pickle(current_path + pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics on reduced dataset\n",
    "n_users = ratingDFX3.userId.unique().shape[0]\n",
    "n_movies = ratingDFX3.movieId.unique().shape[0]\n",
    "n_ratings = len(ratingDFX3)\n",
    "avg_ratings_per_user = n_ratings/n_users\n",
    "\n",
    "print('Number of unique users: ', n_users)\n",
    "print('Number of unique movies: ', n_movies)\n",
    "print('Number of total ratings: ', n_ratings)\n",
    "print('Average number of ratings per user: ', avg_ratings_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into validation and test, such that each is 5% of the dataset\n",
    "X_train, X_test = train_test_split(ratingDFX3, test_size=0.10, \\\n",
    "                                   shuffle=True, random_state=2018)\n",
    "\n",
    "X_validation, X_test = train_test_split(X_test, test_size=0.50, \\\n",
    "                                        shuffle=True, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm size of train, validation, and test datasets\n",
    "print('Size of train set: ', len(X_train))\n",
    "print('Size of validation set: ', len(X_validation))\n",
    "print('Size of test set: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for train\n",
    "ratings_train = np.zeros((n_users, n_movies))\n",
    "for row in X_train.itertuples():\n",
    "    ratings_train[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity of the train ratings matrix\n",
    "sparsity = float(len(ratings_train.nonzero()[0]))\n",
    "sparsity /= (ratings_train.shape[0] * ratings_train.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for validation\n",
    "ratings_validation = np.zeros((n_users, n_movies))\n",
    "for row in X_validation.itertuples():\n",
    "    ratings_validation[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ratings matrix for test\n",
    "ratings_test = np.zeros((n_users, n_movies))\n",
    "for row in X_test.itertuples():\n",
    "    ratings_test[row[6]-1, row[5]-1] = row[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity of the validation ratings matrix\n",
    "sparsity = float(len(ratings_validation.nonzero()[0]))\n",
    "sparsity /= (ratings_validation.shape[0] * ratings_validation.shape[1])\n",
    "sparsity *= 100\n",
    "print('Sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment one - Assign naive 3.5 rating and calculate baseline MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_validation = ratings_validation[ratings_validation.nonzero()].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_validation = np.zeros((len(X_validation),1))\n",
    "pred_validation[pred_validation==0] = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using naive prediction:', naive_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment two - Predict a user's rating based on user's average rating \n",
    "# for all other movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_validation_prediction = np.zeros((n_users, n_movies))\n",
    "i = 0\n",
    "for row in ratings_train:\n",
    "    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n",
    "        = np.mean(row[row>0])\n",
    "    i += 1\n",
    "\n",
    "pred_validation = ratings_validation_prediction \\\n",
    "    [ratings_validation.nonzero()].flatten()\n",
    "user_average = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using user average:', user_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment three - Predict a user's rating for a movie based on the\n",
    "# average rating other users have given that movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_validation_prediction = np.zeros((n_users, n_movies)).T\n",
    "i = 0\n",
    "for row in ratings_train.T:\n",
    "    ratings_validation_prediction[i][ratings_validation_prediction[i]==0] \\\n",
    "        = np.mean(row[row>0])\n",
    "    i += 1\n",
    "\n",
    "ratings_validation_prediction = ratings_validation_prediction.T\n",
    "pred_validation = ratings_validation_prediction \\\n",
    "    [ratings_validation.nonzero()].flatten()\n",
    "movie_average = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using movie average:', movie_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment four - Recommender System using Matrix Factorization\n",
    "# 1 Latent Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 1\n",
    "\n",
    "user_input = Input(shape=[1], name='user')\n",
    "user_embedding = Embedding(input_dim=n_users + 1, \\\n",
    "                           output_dim=n_latent_factors, \\\n",
    "                           name='user_embedding')(user_input)\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "\n",
    "movie_input = Input(shape=[1], name='movie')\n",
    "movie_embedding = Embedding(input_dim=n_movies + 1, \\\n",
    "                            output_dim=n_latent_factors,\n",
    "                            name='movie_embedding')(movie_input)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "product = dot([movie_vec, user_vec], axes=1)\n",
    "model = Model(inputs=[user_input, movie_input], outputs=product)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train.newUserId, X_train.newMovieId], \\\n",
    "                    y=X_train.rating, epochs=100, \\\n",
    "                    validation_data=([X_validation.newUserId, \\\n",
    "                    X_validation.newMovieId], X_validation.rating), \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['val_loss'][10:]).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "print('Minimum MSE: ', min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment five - Recommender System using Matrix Factorization\n",
    "# 3 Latent Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 3\n",
    "\n",
    "user_input = Input(shape=[1], name='user')\n",
    "user_embedding = Embedding(input_dim=n_users + 1, \\\n",
    "                           output_dim=n_latent_factors, \\\n",
    "                           embeddings_regularizer=regularizers.l1(10e-7), \\\n",
    "                           name='user_embedding')(user_input)\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "\n",
    "movie_input = Input(shape=[1], name='movie')\n",
    "movie_embedding = Embedding(input_dim=n_movies + 1, \\\n",
    "                            output_dim=n_latent_factors, \\\n",
    "                            embeddings_regularizer=regularizers.l1(10e-7), \\\n",
    "                            name='movie_embedding')(movie_input)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "product = dot([movie_vec, user_vec], axes=1)\n",
    "model = Model(inputs=[user_input, movie_input], outputs=product)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train.newUserId, X_train.newMovieId], \\\n",
    "                    y=X_train.rating, epochs=100, \\\n",
    "                    validation_data=([X_validation.newUserId, \\\n",
    "                    X_validation.newMovieId], X_validation.rating), \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['val_loss'][10:]).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "print('Minimum MSE: ', min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment six - Recommender System using Matrix Factorization\n",
    "# 5 Latent Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 5\n",
    "\n",
    "user_input = Input(shape=[1], name='user')\n",
    "user_embedding = Embedding(input_dim=n_users + 1, \\\n",
    "                           output_dim=n_latent_factors, \\\n",
    "                           embeddings_regularizer=regularizers.l1(10e-7), \\\n",
    "                           name='user_embedding')(user_input)\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "\n",
    "movie_input = Input(shape=[1], name='movie')\n",
    "movie_embedding = Embedding(input_dim=n_movies + 1, \\\n",
    "                            output_dim=n_latent_factors, \\\n",
    "                            embeddings_regularizer=regularizers.l1(10e-7), \\\n",
    "                            name='movie_embedding')(movie_input)\n",
    "movie_vec = Flatten(name='flatten_movies')(movie_embedding)\n",
    "\n",
    "product = dot([movie_vec, user_vec], axes=1)\n",
    "model = Model(inputs=[user_input, movie_input], outputs=product)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train.newUserId, X_train.newMovieId], \\\n",
    "                    y=X_train.rating, epochs=100, \\\n",
    "                    validation_data=([X_validation.newUserId, \\\n",
    "                    X_validation.newMovieId], X_validation.rating), \\\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(history.history['val_loss'][10:]).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "print('Minimum MSE: ', min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment seven - Recommender System using RBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RBM class\n",
    "class RBM(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, \n",
    "                 learning_rate, epochs, batchsize):\n",
    "        # Define hyperparameters\n",
    "        self._input_size = input_size\n",
    "        self._output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        # Initialize weights and biases using zero matrices\n",
    "        self.w = np.zeros([input_size, output_size], dtype=np.float32)\n",
    "        self.hb = np.zeros([output_size], dtype=np.float32)\n",
    "        self.vb = np.zeros([input_size], dtype=np.float32)\n",
    "\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "\n",
    "    def train(self, X):\n",
    "        _w = tf.placeholder(tf.float32, [self._input_size, self._output_size])\n",
    "        _hb = tf.placeholder(tf.float32, [self._output_size])\n",
    "        _vb = tf.placeholder(tf.float32, [self._input_size])\n",
    "        \n",
    "        prv_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        prv_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        prv_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        cur_w = np.zeros([self._input_size, self._output_size], dtype=np.float32)\n",
    "        cur_hb = np.zeros([self._output_size], dtype=np.float32)\n",
    "        cur_vb = np.zeros([self._input_size], dtype=np.float32)\n",
    "        \n",
    "        v0 = tf.placeholder(tf.float32, [None, self._input_size])\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb))\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "        \n",
    "        update_w = _w + self.learning_rate * \\\n",
    "            (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "        update_vb = _vb +  self.learning_rate * tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb +  self.learning_rate * tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        error_list = []\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), \\\n",
    "                        self.batchsize),range(self.batchsize,len(X), \\\n",
    "                                              self.batchsize)):\n",
    "                    batch = X[start:end]\n",
    "                    cur_w = sess.run(update_w, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict={v0: batch, \\\n",
    "                                    _w: prv_w, _hb: prv_hb, _vb: prv_vb})\n",
    "                    prv_w = cur_w\n",
    "                    prv_hb = cur_hb\n",
    "                    prv_vb = cur_vb\n",
    "                error = sess.run(err, feed_dict={v0: X, \\\n",
    "                                _w: cur_w, _vb: cur_vb, _hb: cur_hb})\n",
    "                print ('Epoch: %d' % epoch,'reconstruction error: %f' % error)\n",
    "                error_list.append(error)\n",
    "            self.w = prv_w\n",
    "            self.hb = prv_hb\n",
    "            self.vb = prv_vb\n",
    "            return error_list\n",
    "\n",
    "    def rbm_output(self, X):\n",
    "        \n",
    "        input_X = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        out = tf.nn.sigmoid(tf.matmul(input_X, _w) + _hb)\n",
    "        hiddenGen = self.sample_prob(self.prob_h_given_v(input_X, _w, _hb))\n",
    "        visibleGen = self.sample_prob(self.prob_v_given_h(hiddenGen, _w, _vb))\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out), sess.run(visibleGen), sess.run(hiddenGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the training cycle\n",
    "\n",
    "# Convert inputX into float32\n",
    "inputX = ratings_train\n",
    "inputX = inputX.astype(np.float32)\n",
    "\n",
    "# Define the parameters of the RBMs we will train\n",
    "rbm=RBM(1000,1000,1,1000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train RBM model\n",
    "err = rbm.train(inputX)\n",
    "outputX, reconstructedX, hiddenX = rbm.rbm_output(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstruction errors\n",
    "pd.Series(err).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reconstruction Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for validation set\n",
    "inputValidation = ratings_validation\n",
    "inputValidation = inputValidation.astype(np.float32)\n",
    "\n",
    "finalOutput_validation, reconstructedOutput_validation, _ = \\\n",
    "    rbm.rbm_output(inputValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE on validation set\n",
    "predictionsArray = reconstructedOutput_validation\n",
    "pred_validation = \\\n",
    "    predictionsArray[ratings_validation.nonzero()].flatten()\n",
    "actual_validation = \\\n",
    "    ratings_validation[ratings_validation.nonzero()].flatten()\n",
    "\n",
    "rbm_prediction = mean_squared_error(pred_validation, actual_validation)\n",
    "print('Mean squared error using RBM prediction:', rbm_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
